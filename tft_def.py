# -*- coding: utf-8 -*-
"""tft_def.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18GGchcp6S0InHJdfYJbXUsJDZYpvC2E1
"""

import copy
import os
import sys
from datetime import datetime
from typing import List, Union
import matplotlib.pyplot as plt
plt.style.use('bmh')
from scipy import signal
import seaborn as sns
from pathlib import Path
import warnings
import pickle
!pip install pytorch_forecasting
!pip install pytorch_lightning
!pip install utils
import lightning.pytorch as pl
from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor
from lightning.pytorch.loggers import TensorBoardLogger
import numpy as np
import pandas as pd
import torch

import pytorch_forecasting
from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet
from pytorch_forecasting.data import GroupNormalizer, encoders, TorchNormalizer
from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss, MultiLoss
from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters
import utils

ts = pd.read_excel("Series_Tesis.xlsx", skiprows=13)

ts.columns = ["time_idx", "agno", "mes", "sal_min","inc", "inf_tot", "inf_met", "pib", "desap", "ap", "ipc", "pob", "exp", "imp", "tmr", "riesgo", "emp", "desemp", "crisis", "pres", "agno_gob"]

print(ts)

ts.drop(["riesgo", "emp", "desemp"], axis=1, inplace=True)

ts.agno_gob = ts.agno_gob.astype(int)
ts.agno = ts.agno.astype(int)
ts.mes = ts.mes.astype(int)
ts.crisis = ts.crisis.astype(int)
ts.inc = ts.inc.astype(int)
ts.time_idx = ts.time_idx.astype(int)
ts.sal_min = ts.sal_min.astype(int)
df = ts

max_prediction_length = 2
max_encoder_length = 13
training_cutoff = df["time_idx"].max() - max_prediction_length

training = TimeSeriesDataSet(
    df[lambda x: x.time_idx <= training_cutoff],
    time_idx="time_idx",
    target="sal_min",
    group_ids = ["pres"],
    min_encoder_length=max_encoder_length // 2,
    max_encoder_length=max_encoder_length,
    min_prediction_length=1,
    max_prediction_length=max_prediction_length,
    time_varying_known_reals=["pob", "inf_met", 'agno_gob', "mes", "time_idx"],
    time_varying_unknown_reals=['ipc', "pib", "exp", "imp", "tmr", "ap", "desap", "exp", "imp", "inf_tot", "sal_min", 'crisis', "inc"],
    time_varying_unknown_categoricals=["pres"],
    target_normalizer=GroupNormalizer(
        groups=["pres"]
    ),
    add_relative_time_idx=True,
    add_encoder_length=True,
    add_target_scales=True,
    allow_missing_timesteps=True,
    categorical_encoders={
        'S100': pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),
        'I100':pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),
        'C100':pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),
        'C101':pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True)
    }
)



validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)

batch_size = 8
train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=6)
val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=6)

early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=1e-4, patience=10, verbose=False, mode="min")
lr_logger = LearningRateMonitor()
logger = TensorBoardLogger("lightning_logs")

actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])
baseline_predictions = Baseline().predict(val_dataloader)
print((actuals - baseline_predictions.to('cpu')).abs().mean().item())

trainer = pl.Trainer(
    max_epochs=50,
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    enable_model_summary=True,
    gradient_clip_val=0.1,
    callbacks=[lr_logger, early_stop_callback],
    logger=logger
)


tft = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=0.15,
    hidden_size=12,
    attention_head_size=4,
    dropout=0.1,
    hidden_continuous_size=6,
    output_size=1,
    loss=SMAPE(),
    reduce_on_plateau_patience=4
)
print(f"Number of parameters in network: {tft.size()/1e3:.1f}k")

trainer.fit(
    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,
)

best_model_path = trainer.checkpoint_callback.best_model_path
best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)

best_model_path

val_prediction_results = best_tft.predict(val_dataloader, mode="raw", return_x=True)

actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])
predictions = best_tft.predict(val_dataloader)

#average p50 loss overall
print((actuals - predictions.to('cpu')).abs().mean().item())
#average p50 loss per time series
print((actuals - predictions.to('cpu')).abs().mean(axis=1))

val_prediction_results = best_tft.predict(
    val_dataloader,
    mode='prediction',
    return_x=True,
    )

predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(val_prediction_results.x, val_prediction_results.output)
features = list(set(predictions_vs_actuals['support'].keys()))
#features = list(set(predictions_vs_actuals['support'].keys())-set(['sal_min_lagged_by_12']))
for feature in features:
    best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals, name=feature);

raw_prediction= best_tft.predict(
    training.filter(lambda x: (x["pres"] == "GP")),
    mode="raw",
    return_x=True,
)

fig, ax = plt.subplots(figsize=(10, 5))
best_tft.plot_prediction(raw_prediction.x, raw_prediction.output, idx=0, ax=ax);

fig, ax = plt.subplots(figsize=(10, 5))

raw_prediction= best_tft.predict(
    training.filter(lambda x: (x.pres == "GP") & (x.time_idx_first_prediction == 10)),
    mode="raw",
    return_x=True,
)
best_tft.plot_prediction(raw_prediction.x, raw_prediction.output, idx=0, ax=ax);

#encoder data is the last lookback window: we get the last 1 week (168 datapoints) for all 5 consumers = 840 total datapoints

encoder_data = df[lambda x: (x.time_idx > (14 - max_encoder_length)) & (x.pres == "GP")]
last_data = df[lambda x: (x.time_idx == 14) & (x.pres == "GP")]


decoder_data = pd.concat(
    [last_data.assign(time_idx=lambda x: x.time_idx + i) for i in range(1, max_prediction_length + 1)],
    ignore_index=True,
)

decoder_data["time_idx"] = (decoder_data["time_idx"] - decoder_data["time_idx"].min())

decoder_data['time_idx'] = decoder_data['time_idx'].astype('int')
decoder_data["time_idx"] += encoder_data["time_idx"].max() + 1 - decoder_data["time_idx"].min()

decoder_data["agno"] = [2023, 2023]
decoder_data["mes"] = [11, 12]
decoder_data["inc"] = [0, 1]

new_prediction_data = pd.concat([df, decoder_data], ignore_index=True)



pred = TimeSeriesDataSet.from_dataset(training, new_prediction_data, predict=True, stop_randomization=True)



fig, ax = plt.subplots(figsize=(10, 5))

#create out-of-sample predictions for MT_002
new_prediction_data=new_prediction_data.query(" pres == 'GP'")

new_raw_predictions = best_tft.predict(new_prediction_data, mode="raw", return_x=True)

best_tft.plot_prediction(new_raw_predictions.x, new_raw_predictions.output, idx=0, show_future_observed=False, ax=ax);

#raw_predictions= best_tft.predict(val_dataloader, mode="raw", return_x=True)

interpretation = best_tft.interpret_output(new_raw_predictions.output, reduction="sum")
best_tft.plot_interpretation(interpretation)